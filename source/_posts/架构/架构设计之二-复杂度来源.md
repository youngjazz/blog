---
title: 架构设计之二--复杂度来源
date: 2018-06-15 11:41:05
tags: 架构设计
categories: 技术
---

在背景阶段我们分析了架构设计的主要目的是解决软件系统中复杂度带来的问题，那么软件复杂度的来源是什么？

## 复杂度来源：高性能

电子计算机从电子管到晶体管到集成电路，运算性能从每秒几次到每秒几亿次。性能越来越高，起复杂度也是越来越高。软件系统也存在这样的现象，Google能够支撑每秒海量搜索，与此同时，软件系统规模也从单台扩展到上万台计算机。当然，技术发展带来的性能提升，不一定会带来复杂度的提升，例如：硬件重纸带->磁带->磁盘->SSD, 并没有带来系统复杂度的显著提升。因为新技术会逐步替代旧技术，这种情况直接选择新技术就好了。但如果不是新技术淘汰旧技术，而是开辟了一个全新的技术领域，才会给软件系统带来复杂度，因为系统设计的时候要在这些技术之间做权衡取舍或者组合。就如同火车不能替代飞机一样，我们需要综合考虑价格、时间、舒适度等因素。

软件系统中高性能的复杂度体现在两个方面：**单机为提高性能带来的复杂度**和**集群为提高性能带来的复杂度**

<!--more-->

### 单机复杂度

计算机内部复杂度最关键的地方就是操作系统。计算机的性能本质上是有硬件发展驱动的，尤其是cpu的性能。操作系统是系统的运行环境，操作系统的复杂度决定软件系统复杂度。

操作系统和性能相关的就是**进程**和**线程**。最初计算机没有操作系统，只有输入、计算和输出功能，为了解决手工输入的低效问题，批处理操作系统应运而生，批处理将指令预先写下来，形成指令清单（就是我们常说的“任务”），操作系统负责读取清单指令并执行，性能大大提高。

但是有一个明显的缺点：批处理程序一次只能执行一个任务，如果某个任务从I/O设备上读取大量数据，在I/O操作过程中，CPU其实是空闲的，而这段时间本来可以用于其他计算。

为了进一步提升性能，人们发明了“进程”，用进程来对应一个任务。每个任务都有自己的独立空间，进程之间互不相关，由操作系统进行调度。此时的CPU还没有多核和多线程的概念，为了达到多进程并行运行，采取了分时的方式。即把CPU的时间分成很多段，每段时间只能处理某个进程的指令。由于CPU处理速度很快，用户感觉到的好像是多进程在并行处理，实际还是串行处理的。

多进程有独立的内存空间，互不相关。但如果能够在运行时通信，会让任务设计变得更加灵活高效。如A任务将结果存储，B再从存储中读取结果进行计算，不仅效率低，而且程序设计复杂。为了解决这个问题，进程间通信方式被设计出来了，包括管道、消息队列、信号量、共享存储等。

多进程让多任务能够并行处理，但本身扔有很多缺点，单个进程内部仍是串行。而实际上进程内部子任务并不要求按严格先后顺序执行，也需要实现并行。如餐厅排队，点餐，服务员调度，收银可以同时践行，不应该出现因为某个顾客结账问题导致别的顾客不能点餐的问题。为此，人们发明了“线程”，线程是进程内部的子任务，但这些子任务共享一份进程数据。为了保证数据正确性，于是发明了**互斥锁机制**。此时操作系统**调度**的最小单位就变成了线程，而进程变成了系统**分配资源**的最小单位。

此时多进程多线程让多任务并行处理的性能大大提升，本质上仍然是分时系统，并不能做到时间上的真正并行。解决这个问题的方式就是让多个cpu同时执行计算。目前这样的解决方案有3种：SMP（Symmetric Multi-Processor，对称多处理器结果）、NUMA（Non-Uniform Memory Access，非一致存储访问结构）、MPP（Massive Parallel Processing，海量并行处理结构）。目前流行的多核处理器就是SMP方案。

然而这些技术不是最新的一定就是最好的，也不是非此即彼的选择。我们需要结合业务进行分析、判断、选择、组合。举个例子：Nginx可以选择多进程也可以选择多线程，JBoss采用多线程，Redis采用单进程，Memcache采用多线程，这些系统都实现了高性能，但内部差异却很大。

### 集群复杂度

随着业务越来越复杂，单台的性能无论如何都不能支撑，必须采用集群的方式达到高性能。但并不是仅仅通过增加机器这么简单，常见方式：

**1.任务分配**

任务分配指的是每个单台机器都能处理完整业务，不同的任务分配到不同的机器上执行。从简单的一台服务器变成两台开始讲解任务分配带来的复杂性，架构示意图如下：

![任务分配架构](http://p7b5cwgjy.bkt.clouddn.com/%E9%9B%86%E7%BE%A4-%E4%BB%BB%E5%8A%A1%E5%88%86%E9%85%8D.png)
相对于单台服务器复杂性体现在：
- 需要额外增加任务分配器器，可以使硬件网络设备(F5、交换机等)，可以是软件(如LVS)，也可能是负载均衡软件(Nginx，HAProxy)或者其他自研系统等。选择合适的任务分配器是一件复杂的事情，需要综合考虑性能、成本、可维护性、可用性等各方面因素。
- 任务分配器和真正的业务服务器之间有连接和交互，需要选择合适的连接方式，并且对连接进行管理。例如：建立连接、连接检测、连接中断后如何处理等。
- 需要增加任务分配算法。是采用轮询、还是按权重分配、或者按照负载进行分配。如果按负载分配，则业务服务器还要上报自己的状态给任务分配器。

上面只是简单一台机器假设能够处理每秒5000次业务请求，两台理论能够处理10000次请求，实际性能一般按照8折大约8000次左右。如果性能要求继续提高，每秒10万次，上面的架构就出现了新的问题，不是简单增加到25台服务器即可，因为随着性能的增加，任务分配器本身又会成为性能瓶颈，单台任务分配器也就不够用了，任务分配器就变成了集群，架构如下：
![复杂的任务分配架构](http://p7b5cwgjy.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-02%2023.12.29.png)

这比两台业务服务器的架构要复杂，主要体现在：
- 任务分配器从一台变为多台，图中用户分配是指将不同的用户分配的不同的分配器上，常见的方法包括DNS轮询、只能DNS、CDN（Content Delivery Network)、GSLB设备(Global Server Load Balance，全局负载均衡)等。
- 任务分配器和业务服务器由1对多变成多对多的网状结构
- 机器从3台扩展到30台(一般任务分配器比业务服务器少，这里假设5-25的比例)，状态管理、故障处理复杂度大大增加
上线的业务不一定就是指某个具体业务，也可能是计算、存储、缓存等。比如Memcache集群架构。
**2.任务分解**
通过任务分配解决了单台服务器性能瓶颈。但如果业务本身越来越复杂，单纯只通过任务分配的方式扩展性能，收益会越来越低。业务简单1台扩展到10台假设可以带了8倍性能提升，而复杂业务可能只能带来5倍提升。造成这种现象的主要原因是业务复杂度越来越高，单台机器处理性能就回越来越低。为了能够继续提升性能，我们采用第二种方式：任务分解。
![任务拆分](http://p7b5cwgjy.bkt.clouddn.com/%E9%9B%86%E7%BE%A4-%E4%BB%BB%E5%8A%A1%E6%8B%86%E5%88%86.png)
将大一统的复杂业务系统，拆分为小而简单的多个系统配合的业务系统。性能提升体现在：
- 简单系统更容易做到高性能。功能越简单，影响性能的点就越少，可以更加针对性的优化。而复杂系统很难找到关键性能点，A关键性能提升了，可能无意减低了B性能，整体性能有可能不升反降。
- 可以针对单个任务进行扩展。拆分独立子系统后，整个系统的瓶颈更容易发现，只需针对瓶颈子系统进行性能优化或提升即可，无需改动整个系统，风险会小很多。如用户增长过快，注册登录子系统出现瓶颈，只需优化注册登录子系统(代码优化或者粗暴加机器)，消息逻辑，LBS可能就无需改动。
**是不是子系统拆的越多越好呢？**其实并不是这样的，这样有可能让系统性能下降，因为系统拆分太细，系统间的调用次数会呈指数级别上升，而系统调用是通过网络传输，性能远比系统内部函数调用低得多。对于架构设计来说，如何把握这个粒度就非常关键了。

## 复杂度来源：高可用

参考维基百科对高可用的定义：

> 系统无中断的执行器功能的能力，代表系统的可用性程度，是进行系统设计时的准则之一。

洽洽难点就在**无中断**上，无论单个硬件还是单个软件都不能无中断，硬件会出故障，软件会出bug；除此之外，外部环境导致的不可用更加不可避免、不受控制。如断电、水灾、地震等而且这些故障影响程度更加严重、而且难以预测和规避。

所以系统高可用方案五花八门，但万变不离其宗，本质上都是“**冗余**”实现高可用。通俗讲就是一台机器不够就两台，一个机房可能断电就两个机房，一个通道可能故障就用两条。单纯从形式上看，是和前面的高性能是一样的都是通过增加机器来解决问题。但本质上又是有根本区别的：**高性能增加机器目的在扩展处理性能，高可用增加机器目的在于冗余处理单元**。正是冗余带来了复杂性：

### 计算高可用

计算有一个特点是：无论在哪台机器上计算，同样的算法和输入数据，输出的结果是一样的。和高性能的双机架构一样，复杂度也是类似的

- 需要增加一个任务分配器，选择合适的任务分配器是一件复杂的事情。需要综合考虑性能、成本、可维护性、可用性等因素
- 任务分配器和业务服务器之间有连接和交互，连接的建立，连接检测，连接中断如何处理
- 任务分配算法。例如常见的双击算法主备、主主，主备可细分冷备、温备、热备。

更复杂些计算高可用集群，分配算法更加复杂，1主3备，2主2备，3主1备，4主0备。具体采用哪种需要结合业务需求分析判断。例如zookeeper采用的是一主多备，而Memcached采用全主0备。

### 存储高可用

恰恰难点就是“存储”，存储和计算的本质区别就是：**将数据从一台机器搬到另一台机器，需要经过线路传输**，线路传输的速度是毫秒级，同机房几毫秒，不同机房几十甚至上百毫秒。如北京到广州机房ping延时大约50ms，不稳定有可能1s还要多。

毫秒对于人类虽然几乎没有感觉，但是对高可用系统就是本质不同了，这意味着整个系统在某个时间点数据是不一致的。按照**数据+逻辑=业务**这个公式来看，数据不一致，即使逻辑一样，最后的业务就不一样了。以银行储蓄为例，A在银行储蓄1万元，北京机房记录该笔存款操作，用户查询余额时数据还没有同步至上海机房，用户体验肯定不好。

除了物理传输速度限制，传输线路本身也存在可用性问题，线路可能中断、线路拥塞、异常（错包，丢包），而且线路故障往往特别长，短的十几分钟，长的几小时都有可能。如2015年支付宝光缆被挖段，影响超过4小时。

所以**存储高可用的难点不在如何备份数据，而在于如何减少或者规避数据不一致对业务的影响**。

### 高可用状态决策

无论计算还是存储高可用，其基础状态都是“**状态决策**”，即判断当前状态是正常还是异常，如果出现了异常就要采取行动保证高可用。如果状态本身就判断错误和偏差，那后续任何行动都是无意义的。但是实践过程中又有这样一个矛盾：**通过冗余来实现的高可用系统，状态决策本质上就不可能做到完全正确。**对常见决策方式进行分析：
**1、独裁式**
**2、协商式**
**3、民主式**

## 复杂度来源：可扩展


## 复杂度来源：低成本、安全、规模